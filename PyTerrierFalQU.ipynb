{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import html\n",
        "\n",
        "def read_answers(xml_file_path):\n",
        "    dic_answer_id_text = {}\n",
        "    tree = ET.parse(xml_file_path)\n",
        "    root = tree.getroot()\n",
        "    for child in root.iter('DOC'):\n",
        "        DOCNO = child[0].text\n",
        "        TEXT = html.unescape(child[1].text)\n",
        "        dic_answer_id_text[DOCNO] = TEXT\n",
        "    return dic_answer_id_text"
      ],
      "metadata": {
        "id": "LwiQoGBNfiSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-terrier\n",
        "!pip install --upgrade git+https://github.com/terrier-org/pyterrier.git"
      ],
      "metadata": {
        "id": "svfTrf0ugYEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYoEZMFhfRq8"
      },
      "outputs": [],
      "source": [
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "  pt.init()\n",
        "\n",
        "# index the collection\n",
        "posts = []\n",
        "dic_answer_id_text = read_answers(\"LawPosts.xml\")\n",
        "for answer_id in dic_answer_id_text:\n",
        "  posts.append({'docno':str(answer_id), 'body': dic_answer_id_text[answer_id]})\n",
        "\n",
        "iter_indexer = pt.IterDictIndexer(\"./index\", meta={'docno': 20, 'body':20000}, overwrite=True)\n",
        "RETRIEVAL_FIELDS = ['body']\n",
        "indexref1 = iter_indexer.index(posts, fields=RETRIEVAL_FIELDS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "4# Load Topics\n",
        "import pandas as pd\n",
        "\n",
        "def read_topics(xml_file_path):\n",
        "    lst_topics = []\n",
        "    tree = ET.parse(xml_file_path)\n",
        "    root = tree.getroot()\n",
        "    for child in root.iter('Question'):\n",
        "        ID = child[0].text\n",
        "        TITLE = html.unescape(child[1].text)\n",
        "        TITLE = \"\".join([x if x.isalnum() else \" \" for x in TITLE])\n",
        "        lst_topics.append([str(ID), TITLE])\n",
        "    return lst_topics\n",
        "\n",
        "lst_topics = read_topics(\"TestTopics.xml\")"
      ],
      "metadata": {
        "id": "TSbZab8DjO-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = pd.DataFrame(lst_topics, columns=['qid','query'])\n",
        "result = pt.BatchRetrieve(indexref1, wmodel=\"TF_IDF\").transform(queries)\n",
        "pt.io.write_results(result, \"res_TF_IDF.txt\", format='trec')\n",
        "print(result)"
      ],
      "metadata": {
        "id": "zj46TY9kiyot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = pd.DataFrame(lst_topics, columns=['qid','query'])\n",
        "result = pt.BatchRetrieve(indexref1, wmodel=\"BM25\").transform(queries)\n",
        "pt.io.write_results(result, \"res_BM25.txt\", format='trec')\n",
        "print(result)"
      ],
      "metadata": {
        "id": "SlA-6ziNnzDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#YAKE\n",
        "!pip install git+https://github.com/LIAAD/yake"
      ],
      "metadata": {
        "id": "_AFzO7A4wttp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yake\n",
        "import pandas as pd\n",
        "kw_extractor = yake.KeywordExtractor(n=1, top=10)\n",
        "\n",
        "\n",
        "def read_topics_yake(xml_file_path):\n",
        "    lst_topics = []\n",
        "    tree = ET.parse(xml_file_path)\n",
        "    root = tree.getroot()\n",
        "    for child in root.iter('Question'):\n",
        "        ID = child[0].text\n",
        "        BODY = html.unescape(child[2].text)\n",
        "        BODY = \"\".join([x if x.isalnum() else \" \" for x in BODY])\n",
        "\n",
        "        TITLE = html.unescape(child[1].text)\n",
        "        TITLE = \"\".join([x if x.isalnum() else \" \" for x in TITLE])\n",
        "\n",
        "        keywords = kw_extractor.extract_keywords(BODY)[:5]\n",
        "        # print(keywords)\n",
        "        Query = \"\"\n",
        "        for kw in keywords:\n",
        "           Query+=kw[0]+\" \"\n",
        "        Query = Query.strip() #+ \" \" + TITLE\n",
        "        Query = Query.strip() + \" \" + TITLE\n",
        "        # print(Query)\n",
        "        lst_topics.append([str(ID), Query])\n",
        "    return lst_topics\n",
        "\n",
        "lst_topics_yake = read_topics_yake(\"TestTopics.xml\")\n"
      ],
      "metadata": {
        "id": "BIwOMiP3w06W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "  pt.init()\n",
        "  \n",
        "queries = pd.DataFrame(lst_topics_yake, columns=['qid','query'])\n",
        "result = pt.BatchRetrieve(indexref1, wmodel=\"TF_IDF\").transform(queries)\n",
        "pt.io.write_results(result, \"res_TF_IDF_YAKE_2.txt\", format='trec')\n",
        "print(result)\n",
        "\n",
        "queries = pd.DataFrame(lst_topics_yake, columns=['qid','query'])\n",
        "result = pt.BatchRetrieve(indexref1, wmodel=\"BM25\").transform(queries)\n",
        "pt.io.write_results(result, \"res_BM25_YAKE_2.txt\", format='trec')\n",
        "print(result)"
      ],
      "metadata": {
        "id": "h2Gxyc4-yDm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ranx"
      ],
      "metadata": {
        "id": "ZmYyyw0trKDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ranx import Qrels, Run, compare, evaluate\n",
        "qrels = Qrels.from_file(\"qrel_test.tsv\", kind=\"trec\")\n",
        "\n",
        "# run_11 = Run.from_file(\"res_TF_IDF.txt\", kind=\"trec\")\n",
        "# run_22 = Run.from_file(\"res_BM25.txt\", kind=\"trec\")\n",
        "run_1 = Run.from_file(\"res_BM25_YAKE_2.txt\", kind=\"trec\")\n",
        "run_2 = Run.from_file(\"res_TF_IDF_YAKE_2.txt\", kind=\"trec\")\n",
        "# run_3 = Run.from_file(\"all-MiniLM-L12-v2_finetuned.tsv\", kind=\"trec\")\n",
        "# run_4 = Run.from_file(\"all-distilroberta-v1_finetuned.tsv\", kind=\"trec\")\n",
        "\n",
        "# run_3 = Run.from_file(\"res_TF_IDF_YAKE.txt\", kind=\"trec\")\n",
        "# run_4 = Run.from_file(\"res_BM25_YAKE.txt\", kind=\"trec\")\n",
        "# run_5 = Run.from_file(\"distilroberta.tsv\", kind=\"trec\")\n",
        "# # run_6 = Run.from_file(\"msmarcodistilbert.tsv\", kind=\"trec\")\n",
        "# run_7 = Run.from_file(\"all-MiniLM-L12-v2 .tsv\", kind=\"trec\")\n",
        "\n",
        "# report = compare(\n",
        "# qrels=qrels,\n",
        "# runs=[run_11, run_22,run_1, run_2, run_3, run_4, run_5, run_7],\n",
        "# metrics=[\"precision@1\", \"mrr@1000\"],\n",
        "# max_p=0.05, # P-value threshold\n",
        "# stat_test=\"student\"\n",
        "# )\n",
        "# report\n",
        "\n",
        "# temp = evaluate(qrels, run, [\"map@100\", \"mrr@10\", \"ndcg@10\"]) # temp is a dictionary\n",
        "# #per query results\n",
        "temp = evaluate(qrels, run_1, [\"precision@1\"], return_mean=False)\n",
        "temp\n",
        "\n",
        "# temp.count(1)"
      ],
      "metadata": {
        "id": "sBvH1TJurTLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SentenceBERT\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "MosFJuSGpg1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses, util, models, evaluation\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "model = SentenceTransformer('all-distilroberta-v1')"
      ],
      "metadata": {
        "id": "AwvcGb-ppvz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses, util, models, evaluation\n",
        "from torch.utils.data import DataLoader\n",
        "import csv \n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "def read_queries(xml_file_path):\n",
        "    dic_topics = {}\n",
        "    tree = ET.parse(xml_file_path)\n",
        "    root = tree.getroot()\n",
        "    for child in root.iter('Question'):\n",
        "        ID = child[0].text\n",
        "        TITLE = html.unescape(child[1].text)\n",
        "        dic_topics[ID] = TITLE\n",
        "    return dic_topics\n",
        "\n",
        "def read_corpus(xml_collection_file_path, xml_file_path):\n",
        "  dic_collection = read_answers(xml_collection_file_path)\n",
        "  dic_queries = read_queries(xml_file_path)\n",
        "  return dic_queries, dic_collection\n",
        "\n",
        "def retrieval():\n",
        "    final_result = {}\n",
        "    print(\"model loaded\")\n",
        "    # concept_map = read_concept_file(\"SQLite.csv\")\n",
        "    \"This is an important part\"\n",
        "    queries, candidates = read_corpus(\"LawPosts.xml\", \"TestTopics.xml\")\n",
        "    print(\"corpus read\")\n",
        "    corpus_embeddings = model.encode(list(candidates.values()), convert_to_tensor=True)\n",
        "    print(\"corpus encoded\")\n",
        "    for topic_id in queries:\n",
        "        temp_dic = {}\n",
        "        query = queries[topic_id]\n",
        "        query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "        hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=1000)\n",
        "        hits = hits[0]  # Get the hits for the first query\n",
        "        for hit in hits:\n",
        "            index = hit['corpus_id']\n",
        "            answer_id = list(candidates.keys())[index]\n",
        "            score = hit['score']\n",
        "            temp_dic[answer_id] = score\n",
        "        final_result[topic_id] = temp_dic\n",
        "    return final_result\n",
        "\n",
        "retrieval_results = retrieval()\n",
        "with open(\"distilroberta.tsv\", mode='w', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    for topic_id in retrieval_results:\n",
        "        result_map = retrieval_results[topic_id]\n",
        "        result_map = dict(sorted(result_map.items(), key=lambda item: item[1], reverse=True))\n",
        "        rank = 1\n",
        "        for post_id in result_map:\n",
        "            score = result_map[post_id]\n",
        "            csv_writer.writerow([topic_id, \"Q0\",  post_id, str(rank), str(score), \"distilroberta\"])\n",
        "            rank += 1\n",
        "            if rank > 1000:\n",
        "                break"
      ],
      "metadata": {
        "id": "72idoJ-Fp5ph"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}